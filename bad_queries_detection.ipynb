{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bf4d0f3-aa33-4186-9519-711141f2f565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "Bad query metrics:\n",
    "1. execution_duration_ms\n",
    "2. total_duration_ms\n",
    "3. compilation_duration_ms\n",
    "<!-- 4. read_partitions -->\n",
    "<!-- 5. pruned_files -->\n",
    "<!-- 6. read_files -->\n",
    "7. read_rows\n",
    "8. produced_rows\n",
    "9. read_bytes\n",
    "10. statement_id\n",
    "11. executed_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea535f3-ea5c-4ee4-a97d-4914b6d6aa62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS ds_training_1.ds_control.metric_weights_bad_queries (\n",
    "#   metric_name VARCHAR(50),\n",
    "#   weight DECIMAL(5, 4)\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771f6d5a-009b-4eed-840a-28217210c5e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# INSERT INTO ds_training_1.ds_control.metric_weights_bad_queries (metric_name, weight) VALUES\n",
    "# (\"Total Duration\", 0.2),\n",
    "# (\"Execution Duration\", 0.2),\n",
    "# (\"Compilation Duration\", 0.1),\n",
    "# (\"Read Rows\", 0.15),\n",
    "# (\"Produced Rows\", 0.15),\n",
    "# (\"Read Bytes\", 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e24ce8b6-ac41-4e84-ac93-796b6f27b736",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a22cedc-700b-452b-9f71-b5f2b3c59ba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM ds_training_1.ds_control.metric_weights_bad_queries;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef589930-989f-4e22-9d29-57b1280f6239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "catalog_schema_df = spark.sql(\"SELECT DISTINCT catalog_name, schema_name FROM ds_training_1.ds_control.table_config\")\n",
    "catalog_schema_df.show()\n",
    "\n",
    "list_of_schemas = [(row.catalog_name, row.schema_name) for row in catalog_schema_df.collect()]\n",
    "print(list_of_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1775132-1416-44b5-a6f3-18e4ba7c7a24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weights_df = spark.sql(\"SELECT metric_name, weight FROM ds_training_1.ds_control.metric_weights_bad_queries\")\n",
    "weights = {row.metric_name: row.weight for row in weights_df.collect()}\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfbbac5-e211-4f5b-96d1-417293163ea4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"num_days\", \"7\", \"Number of Days\")\n",
    "num_days = int(dbutils.widgets.get(\"num_days\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2523f38-92dd-4c19-875f-300ba3928a46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_reqd_metrics_df(num_days):\n",
    "    reqd_metrics = f\"\"\"\n",
    "        SELECT \n",
    "            statement_id,\n",
    "            executed_by,\n",
    "            statement_text,\n",
    "            execution_status,\n",
    "            error_message,\n",
    "            total_duration_ms,\n",
    "            execution_duration_ms,\n",
    "            compilation_duration_ms,\n",
    "            read_rows,\n",
    "            produced_rows,\n",
    "            read_bytes,\n",
    "            end_time \n",
    "        FROM \n",
    "            system.query.history h\n",
    "        JOIN \n",
    "            (SELECT \n",
    "                 '%' || CONCAT(catalog_name, '.', schema_name, '.', table_name) || '%' AS pattern\n",
    "            FROM \n",
    "                ds_training_1.ds_control.table_config) tc\n",
    "        ON \n",
    "            h.statement_text LIKE tc.pattern\n",
    "        WHERE \n",
    "            end_time BETWEEN date_sub(current_date, {num_days}) AND current_date   \n",
    "    \"\"\"\n",
    "\n",
    "    reqd_metrics_df = spark.sql(reqd_metrics.format(num_days=num_days))\n",
    "    return reqd_metrics_df\n",
    "\n",
    "# Assuming `num_days` is defined\n",
    "reqd_metrics_df = get_reqd_metrics_df(num_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74483f56-e1a4-41c3-8aae-a710e1d9c30a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_metrics(reqd_metrics_df):\n",
    "    # Normalize the specified metrics\n",
    "    for metric in [\"total_duration_ms\", \"execution_duration_ms\", \"compilation_duration_ms\", \"read_rows\", \"produced_rows\", \"read_bytes\"]:\n",
    "        max_value = reqd_metrics_df.agg(F.max(metric).alias(\"max\")).collect()[0][\"max\"]\n",
    "        min_value = reqd_metrics_df.agg(F.min(metric).alias(\"min\")).collect()[0][\"min\"]\n",
    "        range_value = max_value - min_value\n",
    "        reqd_metrics_df = reqd_metrics_df.withColumn(f\"normalized_{metric}\", (F.col(metric) - min_value) / range_value)\n",
    "\n",
    "    # Define a window specification to order by end_time to find the most recent execution of the query\n",
    "    window_spec = Window.partitionBy(\"statement_text\").orderBy(F.desc(\"end_time\"))\n",
    "\n",
    "    reqd_metrics_df = (\n",
    "        reqd_metrics_df.withColumn(\"execution_status\", F.first(\"execution_status\").over(window_spec))\n",
    "                  .withColumn(\"error_message\", F.first(\"error_message\").over(window_spec))\n",
    "                  .withColumn(\"executed_by\", F.first(\"executed_by\").over(window_spec))\n",
    "    )\n",
    "    \n",
    "    return reqd_metrics_df\n",
    "\n",
    "# Assuming `num_days` is defined\n",
    "reqd_metrics_df = get_reqd_metrics_df(num_days)\n",
    "reqd_metrics_df = normalize_metrics(reqd_metrics_df)\n",
    "display(reqd_metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cca6594-bf8a-406d-a6de-6c9b146ac365",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_composite_score(reqd_metrics_df, weights):\n",
    "    # Calculate composite score using normalized values\n",
    "    composite_score_df = reqd_metrics_df.groupBy(\"statement_text\").agg(\n",
    "        F.avg(F.coalesce(F.col(\"normalized_total_duration_ms\"), F.lit(1))).alias(\"normalized_total_duration_ms\"),\n",
    "        F.avg(F.coalesce(F.col(\"normalized_execution_duration_ms\"), F.lit(1))).alias(\"normalized_execution_duration_ms\"),\n",
    "        F.avg(F.coalesce(F.col(\"normalized_compilation_duration_ms\"), F.lit(1))).alias(\"normalized_compilation_duration_ms\"),\n",
    "        F.avg(F.coalesce(F.col(\"normalized_read_rows\"), F.lit(0))).alias(\"normalized_read_rows\"),\n",
    "        F.avg(F.coalesce(F.col(\"normalized_produced_rows\"), F.lit(0))).alias(\"normalized_produced_rows\"),\n",
    "        F.avg(F.coalesce(F.col(\"normalized_read_bytes\"), F.lit(0))).alias(\"normalized_read_bytes\"),\n",
    "        F.first(\"execution_status\").alias(\"execution_status\"),\n",
    "        F.first(\"error_message\").alias(\"error_message\"),\n",
    "        F.first(\"executed_by\").alias(\"executed_by\")\n",
    "    ).withColumn(\n",
    "        \"composite_score\",\n",
    "        (weights['Total Duration'] * F.col(\"normalized_total_duration_ms\")) +\n",
    "        (weights['Execution Duration'] * F.col(\"normalized_execution_duration_ms\")) +\n",
    "        (weights['Compilation Duration'] * F.col(\"normalized_compilation_duration_ms\")) +\n",
    "        (weights['Read Rows'] * F.col(\"normalized_read_rows\")) +\n",
    "        (weights['Produced Rows'] * F.col(\"normalized_produced_rows\")) +\n",
    "        (weights['Read Bytes'] * F.col(\"normalized_read_bytes\"))\n",
    "    )\n",
    "\n",
    "    # Rank the queries based on composite score\n",
    "    ranked_df = composite_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"composite_score\"))))\n",
    "\n",
    "    # Calculate min and max composite scores\n",
    "    min_max_scores = composite_score_df.agg(\n",
    "        F.min(\"composite_score\").alias(\"min_score\"),\n",
    "        F.max(\"composite_score\").alias(\"max_score\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    min_score = min_max_scores['min_score']\n",
    "    max_score = min_max_scores['max_score']\n",
    "\n",
    "    # Add a new column for normalized composite score\n",
    "    normalized_score_df = composite_score_df.withColumn(\n",
    "        \"normalized_composite_score\",\n",
    "        (F.col(\"composite_score\") - min_score) / (max_score - min_score)\n",
    "    )\n",
    "\n",
    "    # Rank the queries based on normalized composite score\n",
    "    ranked_normalized_df = normalized_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"normalized_composite_score\"))))\n",
    "\n",
    "    return ranked_normalized_df\n",
    "\n",
    "# Assuming `num_days` and `weights` are defined\n",
    "reqd_metrics_df = get_reqd_metrics_df(num_days)\n",
    "reqd_metrics_df = normalize_metrics(reqd_metrics_df)\n",
    "ranked_normalized_df = calculate_composite_score(reqd_metrics_df, weights)\n",
    "display(ranked_normalized_df.select(\"statement_text\", \"executed_by\",  \"execution_status\", \"error_message\",\"normalized_composite_score\", \"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f0e30041-af6a-426b-a839-009191da4bc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import Window\n",
    "\n",
    "# # variable to convert ms to s and bytes to megabytes\n",
    "# ms_to_s = 1000\n",
    "# b_to_mb = 1e6\n",
    "\n",
    "# reqd_metrics = spark.sql(\"\"\"\n",
    "#         SELECT \n",
    "#             statement_id,\n",
    "#             executed_by,\n",
    "#             statement_text,\n",
    "#             execution_status,\n",
    "#             error_message,\n",
    "#             total_duration_ms,\n",
    "#             execution_duration_ms,\n",
    "#             compilation_duration_ms,\n",
    "#             read_rows,\n",
    "#             produced_rows,\n",
    "#             read_bytes,\n",
    "#             end_time \n",
    "#         FROM \n",
    "#             system.query.history h\n",
    "#         JOIN \n",
    "#             (SELECT \n",
    "#                  '%' || CONCAT(catalog_name, '.', schema_name, '.', table_name) || '%' AS pattern\n",
    "#             FROM \n",
    "#                 ds_training_1.ds_control.table_config) tc\n",
    "#         ON \n",
    "#             h.statement_text LIKE tc.pattern\n",
    "# \"\"\")\n",
    "\n",
    "# # Normalize the specified metrics\n",
    "# for metric in [\"total_duration_ms\", \"execution_duration_ms\", \"compilation_duration_ms\", \"read_rows\", \"produced_rows\", \"read_bytes\"]:\n",
    "#     max_value = reqd_metrics.agg(F.max(metric).alias(\"max\")).collect()[0][\"max\"]\n",
    "#     min_value = reqd_metrics.agg(F.min(metric).alias(\"min\")).collect()[0][\"min\"]\n",
    "#     range_value = max_value - min_value\n",
    "#     reqd_metrics = reqd_metrics.withColumn(f\"normalized_{metric}\", (F.col(metric) - min_value) / range_value)\n",
    "\n",
    "# # Define a window specification to order by end_time to find the most recent execution of the query\n",
    "# window_spec = Window.partitionBy(\"statement_text\").orderBy(F.desc(\"end_time\"))\n",
    "\n",
    "# reqd_metrics_df = (\n",
    "#     reqd_metrics.withColumn(\"execution_status\", F.first(\"execution_status\").over(window_spec))\n",
    "#               .withColumn(\"error_message\", F.first(\"error_message\").over(window_spec))\n",
    "#               .withColumn(\"executed_by\", F.first(\"executed_by\").over(window_spec))\n",
    "# )\n",
    "# display(reqd_metrics_df)\n",
    "\n",
    "# # Calculate composite score using normalized values\n",
    "# composite_score_df = reqd_metrics_df.groupBy(\"statement_text\").agg(\n",
    "#     F.avg(F.coalesce(F.col(\"normalized_total_duration_ms\"), F.lit(1))).alias(\"normalized_total_duration_ms\"),\n",
    "#     F.avg(F.coalesce(F.col(\"normalized_execution_duration_ms\"), F.lit(1))).alias(\"normalized_execution_duration_ms\"),\n",
    "#     F.avg(F.coalesce(F.col(\"normalized_compilation_duration_ms\"), F.lit(1))).alias(\"normalized_compilation_duration_ms\"),\n",
    "#     F.avg(F.coalesce(F.col(\"normalized_read_rows\"), F.lit(0))).alias(\"normalized_read_rows\"),\n",
    "#     F.avg(F.coalesce(F.col(\"normalized_produced_rows\"), F.lit(0))).alias(\"normalized_produced_rows\"),\n",
    "#     F.avg(F.coalesce(F.col(\"normalized_read_bytes\"), F.lit(0))).alias(\"normalized_read_bytes\"),\n",
    "#     F.first(\"execution_status\").alias(\"execution_status\"),\n",
    "#     F.first(\"error_message\").alias(\"error_message\"),\n",
    "#     F.first(\"executed_by\").alias(\"executed_by\")\n",
    "# ).withColumn(\n",
    "#     \"composite_score\",\n",
    "#     (weights['Total Duration'] * F.col(\"normalized_total_duration_ms\")) +\n",
    "#     (weights['Execution Duration'] * F.col(\"normalized_execution_duration_ms\")) +\n",
    "#     (weights['Compilation Duration'] * F.col(\"normalized_compilation_duration_ms\")) +\n",
    "#     (weights['Read Rows'] * F.col(\"normalized_read_rows\")) +\n",
    "#     (weights['Produced Rows'] * F.col(\"normalized_produced_rows\")) +\n",
    "#     (weights['Read Bytes'] * F.col(\"normalized_read_bytes\"))\n",
    "# )\n",
    "# display(composite_score_df)\n",
    "\n",
    "# # Rank the queries based on composite score\n",
    "# ranked_df = composite_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"composite_score\"))))\n",
    "# display(ranked_df)\n",
    "\n",
    "# # Calculate min and max composite scores\n",
    "# min_max_scores = composite_score_df.agg(\n",
    "#     F.min(\"composite_score\").alias(\"min_score\"),\n",
    "#     F.max(\"composite_score\").alias(\"max_score\")\n",
    "# ).collect()[0]\n",
    "\n",
    "# min_score = min_max_scores['min_score']\n",
    "# max_score = min_max_scores['max_score']\n",
    "\n",
    "# # Add a new column for normalized composite score\n",
    "# normalized_score_df = composite_score_df.withColumn(\n",
    "#     \"normalized_composite_score\",\n",
    "#     (F.col(\"composite_score\") - min_score) / (max_score - min_score)\n",
    "# )\n",
    "\n",
    "# # Rank the queries based on normalized composite score\n",
    "# ranked_normalized_df = normalized_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"normalized_composite_score\"))))\n",
    "\n",
    "# display(ranked_normalized_df.select(\"statement_text\", \"executed_by\",  \"execution_status\", \"error_message\",\"normalized_composite_score\", \"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "134712ed-8974-47f5-b386-087ce0452998",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import Window\n",
    "\n",
    "# # variable to convert ms to s and bytes to megabytes\n",
    "# ms_to_s = 1000\n",
    "# b_to_mb = 1e6\n",
    "\n",
    "# reqd_metrics = spark.sql(\"\"\"\n",
    "#         SELECT \n",
    "#             statement_id,\n",
    "#             executed_by,\n",
    "#             statement_text,\n",
    "#             execution_status,\n",
    "#             error_message,\n",
    "#             total_duration_ms,\n",
    "#             execution_duration_ms,\n",
    "#             compilation_duration_ms,\n",
    "#             read_rows,\n",
    "#             produced_rows,\n",
    "#             read_bytes,\n",
    "#             end_time \n",
    "#         FROM \n",
    "#             system.query.history h\n",
    "#         JOIN \n",
    "#             (SELECT \n",
    "#                  '%' || CONCAT(catalog_name, '.', schema_name, '.', table_name) || '%' AS pattern\n",
    "#             FROM \n",
    "#                 ds_training_1.ds_control.table_config) tc\n",
    "#         ON \n",
    "#             h.statement_text LIKE tc.pattern\n",
    "# \"\"\")\n",
    "# display(reqd_metrics)\n",
    "\n",
    "# # Define a window specification to order by end_time to find the most recent execution of the query\n",
    "# window_spec = Window.partitionBy(\"statement_text\").orderBy(F.desc(\"end_time\"))\n",
    "\n",
    "# reqd_metrics_df = (\n",
    "#     reqd_metrics.withColumn(\"execution_status\", F.first(\"execution_status\").over(window_spec))\n",
    "#               .withColumn(\"error_message\", F.first(\"error_message\").over(window_spec))\n",
    "#               .withColumn(\"executed_by\", F.first(\"executed_by\").over(window_spec))\n",
    "# )\n",
    "# display(reqd_metrics_df)\n",
    "\n",
    "\n",
    "# # Calculate composite score\n",
    "# composite_score_df = reqd_metrics_df.groupBy(\"statement_text\").agg(\n",
    "# F.avg(F.coalesce(F.col(\"total_duration_ms\"), F.lit(1000))).alias(\"total_duration_ms\"),\n",
    "# F.avg(F.coalesce(F.col(\"execution_duration_ms\"), F.lit(1000))).alias(\"execution_duration_ms\"),\n",
    "# F.avg(F.coalesce(F.col(\"compilation_duration_ms\"), F.lit(1000))).alias(\"compilation_duration_ms\"),\n",
    "# F.avg(F.coalesce(F.col(\"read_rows\"), F.lit(0))).alias(\"read_rows\"),\n",
    "# F.avg(F.coalesce(F.col(\"produced_rows\"), F.lit(0))).alias(\"produced_rows\"),\n",
    "# F.avg(F.coalesce(F.col(\"read_bytes\"), F.lit(0))).alias(\"read_bytes\"),\n",
    "# F.first(\"execution_status\").alias(\"execution_status\"),\n",
    "# F.first(\"error_message\").alias(\"error_message\"),\n",
    "# F.first(\"executed_by\").alias(\"executed_by\")\n",
    "# ).withColumn(\n",
    "#     \"composite_score\",\n",
    "#     (weights['Total Duration'] * (F.col(\"total_duration_ms\") / ms_to_s)) +\n",
    "#     (weights['Execution Duration'] * (F.col(\"execution_duration_ms\") / ms_to_s)) +\n",
    "#     (weights['Compilation Duration'] * (F.col(\"compilation_duration_ms\") / ms_to_s)) +\n",
    "#     (weights['Read Rows'] * F.col(\"read_rows\")) +\n",
    "#     (weights['Produced Rows'] * F.col(\"produced_rows\")) +\n",
    "#     (weights['Read Bytes'] * (F.col(\"read_bytes\") / b_to_mb))\n",
    "# )\n",
    "# display(composite_score_df)\n",
    "\n",
    "# # Rank the queries based on composite score\n",
    "# ranked_df = composite_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"composite_score\"))))\n",
    "# display(ranked_df.select(\"statement_text\", \"execution_status\", \"error_message\", \"composite_score\", \"rank\"))\n",
    "\n",
    "\n",
    "# # Calculate min and max composite scores\n",
    "# min_max_scores = composite_score_df.agg(\n",
    "#     F.min(\"composite_score\").alias(\"min_score\"),\n",
    "#     F.max(\"composite_score\").alias(\"max_score\")\n",
    "# ).collect()[0]\n",
    "\n",
    "# min_score = min_max_scores['min_score']\n",
    "# max_score = min_max_scores['max_score']\n",
    "\n",
    "# # Add a new column for normalized composite score\n",
    "# normalized_score_df = composite_score_df.withColumn(\n",
    "#     \"normalized_composite_score\",\n",
    "#     (F.col(\"composite_score\") - min_score) / (max_score - min_score)\n",
    "# )\n",
    "\n",
    "# # Rank the queries based on normalized composite score\n",
    "# ranked_normalized_df = normalized_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"normalized_composite_score\"))))\n",
    "\n",
    "# # Show the ranked DataFrame with normalized scores\n",
    "# display(ranked_normalized_df.select(\"statement_text\", \"executed_by\",  \"execution_status\", \"error_message\",\"normalized_composite_score\", \"rank\"))\n",
    "\n",
    "# # Important queries which have failed to get executed\n",
    "# failed_imp_queries = ranked_normalized_df.filter(F.col(\"execution_status\") == \"FAILED\")\n",
    "# display(failed_imp_queries.select(\"statement_text\",  \"executed_by\", \"execution_status\", \"error_message\", \"normalized_composite_score\", \"rank\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732aaea7-1861-4b1b-97f6-073f16bb99b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "copied_df = ranked_normalized_df\n",
    "display(copied_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fc805d9-e8f6-4163-81b9-9f7e290e666e",
     "showTitle": true,
     "title": "USING MEAN AND STANDARD DEVIATION"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "\n",
    "def calculate_threshold_mean_sd(df):\n",
    "    \n",
    "    # Select relevant columns\n",
    "    thresh_df = df.select(\"statement_text\", \"normalized_composite_score\", \"rank\")\n",
    "    \n",
    "    # Collect the normalized composite scores into a list\n",
    "    composite_scores = [row['normalized_composite_score'] for row in thresh_df.collect()]\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    mean_score = np.mean(composite_scores)\n",
    "    std_dev_score = np.std(composite_scores)\n",
    "    \n",
    "    # Set threshold as 2 standard deviations above the mean\n",
    "    threshold = mean_score + 2 * std_dev_score\n",
    "    return threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76efb1ab-2c8a-4eca-965e-88bece66819b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(calculate_threshold_mean_sd(copied_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "947015d6-61c7-4c8d-a83a-72bd3c536c85",
     "showTitle": true,
     "title": "USING ELBOW METHOD"
    }
   },
   "outputs": [],
   "source": [
    "%pip install kneed\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_threshold_elbow_point(thresh_df):\n",
    "\n",
    "    # Assuming composite_scores are sorted in descending order and collected from the DataFrame\n",
    "    composite_scores = [row['normalized_composite_score'] for row in thresh_df.orderBy(F.desc(\"normalized_composite_score\")).collect()]\n",
    "\n",
    "    ranks = list(range(1, len(composite_scores) + 1))\n",
    "\n",
    "    # Plot the rank vs composite score for visualization\n",
    "    plt.plot(ranks, composite_scores)\n",
    "    plt.xlabel('Rank')\n",
    "    plt.ylabel('Composite Score')\n",
    "    plt.title('Elbow Method for Threshold Selection')\n",
    "    plt.show()\n",
    "\n",
    "    # Use KneeLocator to find the elbow point\n",
    "    knee_locator = KneeLocator(ranks, composite_scores, curve='convex', direction='decreasing')\n",
    "\n",
    "    # Return the composite score\n",
    "    return composite_scores[knee_locator.knee - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6cb52f4-846b-454b-a931-5cb7124495d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(calculate_threshold_elbow_point(copied_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a41d47bd-ad0c-4993-8cda-8273cf714f09",
     "showTitle": true,
     "title": "USING K-MEANS"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def calculate_threshold_k_means(thresh_df):\n",
    "\n",
    "    # Convert the Spark DataFrame to a Pandas DataFrame and extract the numpy array\n",
    "    composite_scores = np.array(thresh_df.select(\"normalized_composite_score\").toPandas()).reshape(-1, 1)\n",
    "\n",
    "    # Perform K-means clustering with 2 clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(composite_scores)\n",
    "\n",
    "    # Identify the cluster centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Choose the higher centroid as the \"bad query\" cluster threshold\n",
    "    threshold = max(centroids)[0]\n",
    "\n",
    "    # Visualize the clusters\n",
    "    plt.scatter(composite_scores, np.zeros_like(composite_scores), c=kmeans.labels_, cmap='rainbow')\n",
    "    plt.scatter(centroids[:, 0], [0, 0], color='black', marker='x', s=100, label='Centroids')\n",
    "    plt.title('K-means Clustering of Composite Scores')\n",
    "    plt.xlabel('Normalized Composite Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4348beb5-bf9f-4600-9ed0-68b1186df3ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(calculate_threshold_k_means(copied_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f72acf7-9e3f-4e83-ab59-b88fd94eed22",
     "showTitle": true,
     "title": "USING PERCENTILE"
    }
   },
   "outputs": [],
   "source": [
    "def calulate_threshold_percentile(thresh_df):\n",
    "    # Calculate the 90th percentile threshold\n",
    "    threshold = thresh_df.approxQuantile(\"normalized_composite_score\", [0.9], 0.0)[0]\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d244c0f3-1782-4bf8-8dbf-c2bd6163e16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(calulate_threshold_percentile(copied_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c1580f4-ee1e-45f5-92e2-d90b75261d80",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Classifying good and bad queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17da1e62-3349-44f6-ac16-af0249af936f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a method to dynamically select the threshold calculation method\n",
    "def calculate_dynamic_threshold(method, df):\n",
    "    if method == \"mean_sd\":\n",
    "        return calculate_threshold_mean_sd(df)\n",
    "    elif method == \"elbow_point\":\n",
    "        return calculate_threshold_elbow_point(df)\n",
    "    elif method == \"k_means\":\n",
    "        return calculate_threshold_k_means(df)\n",
    "    elif method == \"percentile\":\n",
    "        return calulate_threshold_percentile(df)\n",
    "    else:\n",
    "        return 0.4\n",
    "\n",
    "# User can select the method here\n",
    "selected_method = \"\"  # Options: \"mean_sd\", \"elbow_point\", \"k_means\", \"percentile\"\n",
    "\n",
    "# Call the dynamic threshold calculation function\n",
    "comp_score_thresh = calculate_dynamic_threshold(selected_method, copied_df)\n",
    "display(comp_score_thresh)\n",
    "\n",
    "# Add the new column 'type_of_query' with conditions applied only to \"FINISHED\" queries, default to \"Good Query\"\n",
    "copied_df = copied_df.withColumn(\n",
    "    \"type_of_query\",\n",
    "    F.when(\n",
    "        (F.col(\"normalized_composite_score\") > comp_score_thresh) & \n",
    "        (F.col(\"execution_status\") == \"FINISHED\"), \n",
    "        \"Bad Query\"\n",
    "    ).when(\n",
    "        F.col(\"execution_status\") == \"FINISHED\",\n",
    "        \"Good Query\"\n",
    "    ).otherwise(\"Not Applicable\")\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "display(copied_df.select(\"statement_text\", \"executed_by\", \"execution_status\", \"error_message\", \"normalized_composite_score\", \"rank\", \"type_of_query\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5c3c10c9-44ea-4e08-8482-adf604009e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the threshold\n",
    "# comp_score_thresh = 0.40\n",
    "\n",
    "# # Add the new column 'type_of_query' with conditions applied only to \"FINISHED\" queries, default to \"Good Query\"\n",
    "# ranked_normalized_df = ranked_normalized_df.withColumn(\n",
    "#     \"type_of_query\",\n",
    "#     F.when(\n",
    "#         (F.col(\"normalized_composite_score\") > comp_score_thresh) & \n",
    "#         (F.col(\"execution_status\") == \"FINISHED\"), \n",
    "#         \"Bad Query\"\n",
    "#     ).when(\n",
    "#         F.col(\"execution_status\") == \"FINISHED\",\n",
    "#         \"Good Query\"\n",
    "#     ).otherwise(\"Not Applicable\")\n",
    "# )\n",
    "\n",
    "# # Display the updated DataFrame\n",
    "# display(ranked_normalized_df.select(\"statement_text\", \"executed_by\", \"execution_status\", \"error_message\", \"normalized_composite_score\", \"rank\", \"type_of_query\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "659a719f-21fd-428a-9514-74dc8164e4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "top_10_bad_queries=copied_df.filter(F.col(\"type_of_query\") == \"Bad Query\").orderBy(F.col(\"rank\").asc()).limit(10)\n",
    "display(top_10_bad_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b2c8bf-14be-4b79-acac-e5dce08b01c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bad_queries = [row['statement_text'] for row in top_4_bad_queries.select(\"statement_text\").collect()]\n",
    "print(bad_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa221eb8-917c-47dd-b532-83cd1c6785a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "params = {\n",
    "    \"bad_queries\" : bad_queries\n",
    "    \n",
    "}\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f80e4b04-dc04-45c2-83bf-b618b42c4d7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(\"params\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "576412e9-370c-4021-98bc-ae3b71754dcd",
     "showTitle": true,
     "title": "USING ELBOW METHOD"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install kneed\n",
    "# from kneed import KneeLocator\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming composite_scores are sorted in descending order and collected from the DataFrame\n",
    "# composite_scores = [row['normalized_composite_score'] for row in thresh_df.orderBy(F.desc(\"normalized_composite_score\")).collect()]\n",
    "\n",
    "# ranks = list(range(1, len(composite_scores) + 1))\n",
    "\n",
    "# # Plot the rank vs composite score for visualization\n",
    "# plt.plot(ranks, composite_scores)\n",
    "# plt.xlabel('Rank')\n",
    "# plt.ylabel('Composite Score')\n",
    "# plt.title('Elbow Method for Threshold Selection')\n",
    "# plt.show()\n",
    "\n",
    "# # Use KneeLocator to find the elbow point\n",
    "# knee_locator = KneeLocator(ranks, composite_scores, curve='convex', direction='decreasing')\n",
    "\n",
    "# # Print the elbow point\n",
    "# print(f\"The elbow point is at rank: {knee_locator.knee}, with a composite score of: {composite_scores[knee_locator.knee - 1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "55b811c0-b25e-4c34-a6d9-86bb22d2b585",
     "showTitle": true,
     "title": "USING K-MEANS CLUSTERING"
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Convert the Spark DataFrame to a Pandas DataFrame and extract the numpy array\n",
    "# composite_scores = np.array(thresh_df.select(\"normalized_composite_score\").toPandas()).reshape(-1, 1)\n",
    "\n",
    "# # Perform K-means clustering with 2 clusters\n",
    "# from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=2, random_state=0).fit(composite_scores)\n",
    "\n",
    "# # Identify the cluster centroids\n",
    "# centroids = kmeans.cluster_centers_\n",
    "\n",
    "# # Choose the higher centroid as the \"bad query\" cluster threshold\n",
    "# threshold = max(centroids)[0]\n",
    "# display(threshold)\n",
    "\n",
    "# # Visualize the clusters\n",
    "# plt.scatter(composite_scores, np.zeros_like(composite_scores), c=kmeans.labels_, cmap='rainbow')\n",
    "# plt.scatter(centroids[:, 0], [0, 0], color='black', marker='x', s=100, label='Centroids')\n",
    "# plt.title('K-means Clustering of Composite Scores')\n",
    "# plt.xlabel('Normalized Composite Score')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Filter thresh_df to include only bad queries based on the threshold\n",
    "# bad_queries_df = thresh_df.filter(thresh_df.normalized_composite_score >= threshold)\n",
    "\n",
    "# display(bad_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c66f5ed9-eba8-4ae4-b72d-c759e8ef62db",
     "showTitle": true,
     "title": "USING PERCENTILE"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import lit, when\n",
    "\n",
    "# # Calculate the 90th percentile threshold\n",
    "# threshold = thresh_df.approxQuantile(\"normalized_composite_score\", [0.9], 0.0)[0]\n",
    "# display(threshold)\n",
    "\n",
    "# # Label as bad queries if above threshold\n",
    "# bad_queries_df = thresh_df.withColumn(\"is_bad_query\", when(thresh_df.normalized_composite_score >= threshold, lit(True)).otherwise(lit(False)))\n",
    "\n",
    "# display(bad_queries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e3bb7424-832d-4756-aa10-7de791dd0ee6",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql import Window \n",
    "\n",
    "# for i in range(len(list_of_schemas)):\n",
    "\n",
    "#     print(list_of_schemas[i][0], list_of_schemas[i][1])\n",
    "\n",
    "#     reqd_metrics = spark.sql(f\"\"\"\n",
    "#     SELECT \n",
    "#         statement_id,\n",
    "#         executed_by,\n",
    "#         statement_text,\n",
    "#         total_duration_ms,\n",
    "#         execution_duration_ms,\n",
    "#         compilation_duration_ms,\n",
    "#         read_rows,\n",
    "#         produced_rows,\n",
    "#         read_bytes \n",
    "#     FROM system.query.history \n",
    "#     WHERE statement_text LIKE '%{list_of_schemas[i][0]}.{list_of_schemas[i][1]}%';\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Define weights\n",
    "#     w1 = 0.2  # total_duration_ms\n",
    "#     w2 = 0.2  # execution_duration_ms\n",
    "#     w3 = 0.1  # compilation_duration_ms\n",
    "#     w4 = 0.15  # read_rows\n",
    "#     w5 = 0.15  # produced_rows\n",
    "#     w6 = 0.2  # read_bytes\n",
    "\n",
    "\n",
    "# # Calculate composite score\n",
    "#     composite_score_df = reqd_metrics.groupBy(\"statement_text\").agg(\n",
    "#         F.avg(F.coalesce(F.col(\"total_duration_ms\"), F.lit(1000))).alias(\"total_duration_ms\"),\n",
    "#         F.avg(F.coalesce(F.col(\"execution_duration_ms\"), F.lit(1000))).alias(\"execution_duration_ms\"),\n",
    "#         F.avg(F.coalesce(F.col(\"compilation_duration_ms\"), F.lit(1000))).alias(\"compilation_duration_ms\"),\n",
    "#         F.avg(F.coalesce(F.col(\"read_rows\"), F.lit(0))).alias(\"read_rows\"),\n",
    "#         F.avg(F.coalesce(F.col(\"produced_rows\"), F.lit(0))).alias(\"produced_rows\"),\n",
    "#         F.avg(F.coalesce(F.col(\"read_bytes\"), F.lit(0))).alias(\"read_bytes\")\n",
    "#     ).withColumn(\n",
    "#         \"composite_score\",\n",
    "#         (w1 * (F.col(\"total_duration_ms\") / 1000)) +\n",
    "#         (w2 * (F.col(\"execution_duration_ms\") / 1000)) +\n",
    "#         (w3 * (F.col(\"compilation_duration_ms\") / 1000)) +\n",
    "#         (w4 * F.col(\"read_rows\")) +\n",
    "#         (w5 * F.col(\"produced_rows\")) +\n",
    "#         (w6 * (F.col(\"read_bytes\") / 1e6))\n",
    "#     )\n",
    "\n",
    "#     display(composite_score_df)\n",
    "\n",
    "#     # Filter to unique statement_text entries before ranking\n",
    "#     unique_statements_df = composite_score_df.select(\"statement_text\", \"composite_score\").distinct()\n",
    "\n",
    "#      # Rank the queries based on composite score\n",
    "#     ranked_df = unique_statements_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"composite_score\"))))\n",
    "\n",
    "#     # Show the ranked DataFrame\n",
    "#     display(ranked_df.select(\"statement_text\", \"composite_score\", \"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8b9ded-ac53-4cdb-adc1-afb32345dde4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Save the DataFrame as a Delta table\n",
    "# reqd_metrics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"ds_training_1.ds_gold.bad_qd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63da48fb-8e39-4711-9847-e016e0e7dd67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# -- Use SQL to insert data from the temporary Delta table into another table\n",
    "# INSERT INTO ds_training_1.ds_gold.bad_queries_data\n",
    "# SELECT statement_id, executed_by, statement_text, total_duration_ms, execution_duration_ms, compilation_duration_ms, read_rows, produced_rows, read_bytes FROM ds_training_1.ds_gold.bad_qd;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83dac4f4-692d-4e4f-adc7-61a36ab3ac33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# variable to convert ms to s and bytes to megabytes\n",
    "ms_to_s = 1000\n",
    "b_to_mb = 1e6\n",
    "\n",
    "reqd_metrics = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            statement_id,\n",
    "            executed_by,\n",
    "            statement_text,\n",
    "            execution_status,\n",
    "            error_message,\n",
    "            total_duration_ms,\n",
    "            execution_duration_ms,\n",
    "            compilation_duration_ms,\n",
    "            read_rows,\n",
    "            produced_rows,\n",
    "            read_bytes,\n",
    "            end_time \n",
    "        FROM \n",
    "            system.query.history h\n",
    "        JOIN \n",
    "            (SELECT \n",
    "                 '%' || CONCAT(catalog_name, '.', schema_name, '.', table_name) || '%' AS pattern\n",
    "            FROM \n",
    "                ds_training_1.ds_control.table_config) tc\n",
    "        ON \n",
    "            h.statement_text LIKE tc.pattern\n",
    "        WHERE \n",
    "            end_time BETWEEN date_sub(current_date, 7) AND current_date   \n",
    "            \n",
    "\"\"\")\n",
    "\n",
    "# Normalize the specified metrics\n",
    "for metric in [\"total_duration_ms\", \"execution_duration_ms\", \"compilation_duration_ms\", \"read_rows\", \"produced_rows\", \"read_bytes\"]:\n",
    "    max_value = reqd_metrics.agg(F.max(metric).alias(\"max\")).collect()[0][\"max\"]\n",
    "    min_value = reqd_metrics.agg(F.min(metric).alias(\"min\")).collect()[0][\"min\"]\n",
    "    range_value = max_value - min_value\n",
    "    reqd_metrics = reqd_metrics.withColumn(f\"normalized_{metric}\", (F.col(metric) - min_value) / range_value)\n",
    "\n",
    "# Define a window specification to order by end_time to find the most recent execution of the query\n",
    "window_spec = Window.partitionBy(\"statement_text\").orderBy(F.desc(\"end_time\"))\n",
    "\n",
    "reqd_metrics_df = (\n",
    "    reqd_metrics.withColumn(\"execution_status\", F.first(\"execution_status\").over(window_spec))\n",
    "              .withColumn(\"error_message\", F.first(\"error_message\").over(window_spec))\n",
    "              .withColumn(\"executed_by\", F.first(\"executed_by\").over(window_spec))\n",
    ")\n",
    "display(reqd_metrics_df)\n",
    "\n",
    "# Calculate composite score using normalized values\n",
    "composite_score_df = reqd_metrics_df.groupBy(\"statement_text\").agg(\n",
    "    F.avg(F.coalesce(F.col(\"normalized_total_duration_ms\"), F.lit(1))).alias(\"normalized_total_duration_ms\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_execution_duration_ms\"), F.lit(1))).alias(\"normalized_execution_duration_ms\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_compilation_duration_ms\"), F.lit(1))).alias(\"normalized_compilation_duration_ms\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_read_rows\"), F.lit(0))).alias(\"normalized_read_rows\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_produced_rows\"), F.lit(0))).alias(\"normalized_produced_rows\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_read_bytes\"), F.lit(0))).alias(\"normalized_read_bytes\"),\n",
    "    F.first(\"execution_status\").alias(\"execution_status\"),\n",
    "    F.first(\"error_message\").alias(\"error_message\"),\n",
    "    F.first(\"executed_by\").alias(\"executed_by\")\n",
    ").withColumn(\n",
    "    \"composite_score\",\n",
    "    (weights['Total Duration'] * F.col(\"normalized_total_duration_ms\")) +\n",
    "    (weights['Execution Duration'] * F.col(\"normalized_execution_duration_ms\")) +\n",
    "    (weights['Compilation Duration'] * F.col(\"normalized_compilation_duration_ms\")) +\n",
    "    (weights['Read Rows'] * F.col(\"normalized_read_rows\")) +\n",
    "    (weights['Produced Rows'] * F.col(\"normalized_produced_rows\")) +\n",
    "    (weights['Read Bytes'] * F.col(\"normalized_read_bytes\"))\n",
    ")\n",
    "display(composite_score_df)\n",
    "\n",
    "# Rank the queries based on composite score\n",
    "ranked_df = composite_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"composite_score\"))))\n",
    "display(ranked_df)\n",
    "\n",
    "# Calculate min and max composite scores\n",
    "min_max_scores = composite_score_df.agg(\n",
    "    F.min(\"composite_score\").alias(\"min_score\"),\n",
    "    F.max(\"composite_score\").alias(\"max_score\")\n",
    ").collect()[0]\n",
    "\n",
    "min_score = min_max_scores['min_score']\n",
    "max_score = min_max_scores['max_score']\n",
    "\n",
    "# Add a new column for normalized composite score\n",
    "normalized_score_df = composite_score_df.withColumn(\n",
    "    \"normalized_composite_score\",\n",
    "    (F.col(\"composite_score\") - min_score) / (max_score - min_score)\n",
    ")\n",
    "\n",
    "# Rank the queries based on normalized composite score\n",
    "ranked_normalized_df = normalized_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"normalized_composite_score\"))))\n",
    "\n",
    "display(ranked_normalized_df.select(\"statement_text\", \"executed_by\",  \"execution_status\", \"error_message\",\"normalized_composite_score\", \"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee82604-a2ad-4f09-adfe-c4cfffe16211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "df = copied_df.select(\"statement_text\", \"executed_by\", \"execution_status\", \"error_message\", \"normalized_composite_score\", \"rank\", \"type_of_query\")\n",
    "df.write.saveAsTable(\"ds_training_1.ds_gold.bad_query_final\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894aeb43-dbd7-4985-837c-b317ae0e8ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8762ed7d-b196-4271-a7de-e8ee1c506276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# variable to convert ms to s and bytes to megabytes\n",
    "ms_to_s = 1000\n",
    "b_to_mb = 1e6\n",
    "\n",
    "reqd_metrics = f\"\"\"\n",
    "        SELECT \n",
    "            statement_id,\n",
    "            executed_by,\n",
    "            statement_text,\n",
    "            execution_status,\n",
    "            error_message,\n",
    "            total_duration_ms,\n",
    "            execution_duration_ms,\n",
    "            compilation_duration_ms,\n",
    "            read_rows,\n",
    "            produced_rows,\n",
    "            read_bytes,\n",
    "            end_time \n",
    "        FROM \n",
    "            system.query.history h\n",
    "        JOIN \n",
    "            (SELECT \n",
    "                 '%' || CONCAT(catalog_name, '.', schema_name, '.', table_name) || '%' AS pattern\n",
    "            FROM \n",
    "                ds_training_1.ds_control.table_config) tc\n",
    "        ON \n",
    "            h.statement_text LIKE tc.pattern\n",
    "        WHERE \n",
    "            end_time BETWEEN date_sub(current_date, {num_days}) AND current_date   \n",
    "\"\"\"\n",
    "\n",
    "# Assuming `spark` is your SparkSession and `num_days` is defined\n",
    "reqd_metrics_df = spark.sql(reqd_metrics.format(num_days=num_days))\n",
    "\n",
    "# Normalize the specified metrics\n",
    "for metric in [\"total_duration_ms\", \"execution_duration_ms\", \"compilation_duration_ms\", \"read_rows\", \"produced_rows\", \"read_bytes\"]:\n",
    "    max_value = reqd_metrics_df.agg(F.max(metric).alias(\"max\")).collect()[0][\"max\"]\n",
    "    min_value = reqd_metrics_df.agg(F.min(metric).alias(\"min\")).collect()[0][\"min\"]\n",
    "    range_value = max_value - min_value\n",
    "    reqd_metrics_df = reqd_metrics_df.withColumn(f\"normalized_{metric}\", (F.col(metric) - min_value) / range_value)\n",
    "\n",
    "# Define a window specification to order by end_time to find the most recent execution of the query\n",
    "window_spec = Window.partitionBy(\"statement_text\").orderBy(F.desc(\"end_time\"))\n",
    "\n",
    "reqd_metrics_df = (\n",
    "    reqd_metrics_df.withColumn(\"execution_status\", F.first(\"execution_status\").over(window_spec))\n",
    "              .withColumn(\"error_message\", F.first(\"error_message\").over(window_spec))\n",
    "              .withColumn(\"executed_by\", F.first(\"executed_by\").over(window_spec))\n",
    ")\n",
    "display(reqd_metrics_df)\n",
    "\n",
    "# Calculate composite score using normalized values\n",
    "composite_score_df = reqd_metrics_df.groupBy(\"statement_text\").agg(\n",
    "    F.avg(F.coalesce(F.col(\"normalized_total_duration_ms\"), F.lit(1))).alias(\"normalized_total_duration_ms\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_execution_duration_ms\"), F.lit(1))).alias(\"normalized_execution_duration_ms\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_compilation_duration_ms\"), F.lit(1))).alias(\"normalized_compilation_duration_ms\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_read_rows\"), F.lit(0))).alias(\"normalized_read_rows\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_produced_rows\"), F.lit(0))).alias(\"normalized_produced_rows\"),\n",
    "    F.avg(F.coalesce(F.col(\"normalized_read_bytes\"), F.lit(0))).alias(\"normalized_read_bytes\"),\n",
    "    F.first(\"execution_status\").alias(\"execution_status\"),\n",
    "    F.first(\"error_message\").alias(\"error_message\"),\n",
    "    F.first(\"executed_by\").alias(\"executed_by\")\n",
    ").withColumn(\n",
    "    \"composite_score\",\n",
    "    (weights['Total Duration'] * F.col(\"normalized_total_duration_ms\")) +\n",
    "    (weights['Execution Duration'] * F.col(\"normalized_execution_duration_ms\")) +\n",
    "    (weights['Compilation Duration'] * F.col(\"normalized_compilation_duration_ms\")) +\n",
    "    (weights['Read Rows'] * F.col(\"normalized_read_rows\")) +\n",
    "    (weights['Produced Rows'] * F.col(\"normalized_produced_rows\")) +\n",
    "    (weights['Read Bytes'] * F.col(\"normalized_read_bytes\"))\n",
    ")\n",
    "display(composite_score_df)\n",
    "\n",
    "# Rank the queries based on composite score\n",
    "ranked_df = composite_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"composite_score\"))))\n",
    "display(ranked_df)\n",
    "\n",
    "# Calculate min and max composite scores\n",
    "min_max_scores = composite_score_df.agg(\n",
    "    F.min(\"composite_score\").alias(\"min_score\"),\n",
    "    F.max(\"composite_score\").alias(\"max_score\")\n",
    ").collect()[0]\n",
    "\n",
    "min_score = min_max_scores['min_score']\n",
    "max_score = min_max_scores['max_score']\n",
    "\n",
    "# Add a new column for normalized composite score\n",
    "normalized_score_df = composite_score_df.withColumn(\n",
    "    \"normalized_composite_score\",\n",
    "    (F.col(\"composite_score\") - min_score) / (max_score - min_score)\n",
    ")\n",
    "\n",
    "# Rank the queries based on normalized composite score\n",
    "ranked_normalized_df = normalized_score_df.withColumn(\"rank\", F.row_number().over(Window.orderBy(F.desc(\"normalized_composite_score\"))))\n",
    "\n",
    "display(ranked_normalized_df.select(\"statement_text\", \"executed_by\",  \"execution_status\", \"error_message\",\"normalized_composite_score\", \"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8d041f8-be00-4e96-91ed-0db11b8ee0ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3781786382400082,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bad_queries_detection",
   "widgets": {
    "num_days": {
     "currentValue": "100",
     "nuid": "cdb93ce7-cb95-44e7-9a08-5e38b3842cf0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "7",
      "label": "Number of Days",
      "name": "num_days",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "7",
      "label": "Number of Days",
      "name": "num_days",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
